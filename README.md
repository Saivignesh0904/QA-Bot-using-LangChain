

# QA Bot with LangChain, LLM & Gradio

A question-answering (QA) bot that loads your documents (PDF, DOCX, TXT, etc.), indexes them with embeddings, and answers natural-language questions based **only** on the content of those documents. Built with **LangChain**, an **LLM backend** (IBM watsonx AI), a **vector database**, and a **Gradio** web UI.

---

## âœ¨ Features

- ðŸ” **Document-aware QA** â€“ Answers questions using only your loaded files
- ðŸ“š **Multi-format document loader** â€“ PDFs, Word documents, plain text, etc.
- ðŸ§© **Smart text chunking** â€“ Splits long documents into meaningful chunks
- ðŸ§  **Embeddings & vector store** â€“ Semantic search over your docs
- ðŸ”Ž **Retriever pipeline** â€“ Retrieves only relevant chunks per query
- ðŸ’¬ **LLM-powered answers** â€“ Uses a large language model for natural, contextual responses
- ðŸŒ **Gradio front-end** â€“ Clean web UI for uploading docs and chatting with the bot
- ðŸ“ˆ **Scalable design** â€“ Easily extend with more docs or a different vector database / LLM

---

## ðŸ§± Architecture Overview

End-to-end flow:

1. **Document Loader**  
   Load source documents (PDF, DOCX, TXT, etc.) into memory.

2. **Text Splitter**  
   Split long documents into smaller, overlapping text chunks to make embedding and retrieval efficient.

3. **Embeddings**  
   Convert each chunk into a numerical vector using an embedding model (OpenAI, watsonx, or other).

4. **Vector Database**  
   Store vectors in a vector store (e.g., FAISS, Chroma, etc.) for efficient similarity search.

5. **Retriever**  
   Given a user question, retrieve top-k most relevant chunks from the vector database.

6. **LLM Question Answering**  
   Feed the retrieved chunks + question into an LLM via LangChain to generate a final answer.

7. **Gradio Front-end**  
   Provide a simple web interface where users:
   - Upload documents
   - Ask questions
   - See answers and (optionally) source snippets

---

## ðŸ§° Tech Stack

- **Language:** Python
- **Framework:** [LangChain](https://python.langchain.com/)
- **LLM Backend (examples):**
  - OpenAI GPT models
  - IBM watsonx AI LLM and embedding APIs :contentReference[oaicite:1]{index=1}
- **Vector Store:** (example) FAISS / Chroma / other LangChain-compatible store
- **UI:** [Gradio](https://www.gradio.app/)
- **Environment:** `pip` / `virtualenv` / `conda`

> ðŸ’¡ You can swap the LLM and vector DB as long as they are supported by LangChain.

---


